
Artificial Intelligence for Business Leaders

[![GitPitch](https://gitpitch.com/assets/badge.svg)](https://gitpitch.com/fractus-io/ai4bl/master?grs=github&t=white)


Table of Contents

#### Introduction

* [What is AI](#WhatIsArtificialIntelligence)
  * [What is Artificial Intelligence](#WhatIsArtificialIntelligence)
  * [Applications of AI](#ApplicationsOfAI)
  * [Companies that use AI](#CompaniesThatUseAI)
  * [The future prospect of AI](#TheFutureProspectOfAI)
* [History of AI](#HistoryofAI)
  * [Introduction](#hoai-Introduction)
  * [The rise of robotics and AI](#TheRiseOfRoboticsAndAI)
  * [The AI Winters](#TheAIWinters)
  * [New Opportunities for AI](#NewOpportunitiesForAI)
* [Future of the AI](#FutureofAI)
  * [Introduction](#foai-Introduction)
  * [The future of AI – beyond expectation](#TheFutureOfAIBeyondExpectation)
  * [Computing Power](#ComputingPower)
  * [Emerging Technology](#EmergingTechnology)
  * [Assistants will become predictive](#AssistantsWillBecomePredictive)
  * [Affective Computing](#AffectiveComputing) 
  * [Changing Professions](#ChangingProfessions) 
  * [Let’s get digital](#LetsGetDigital)

#### Understand AI

* [AI, Machine Learning and Deep Learning](/tutorials/ai4bl/ai-machine-learning-and-deep-learning)
	* [Introduction](#Introduction)
	* [What Is Machine Learning ?](#WhatIsMachineLearning)
	* [What Is Deep Learning ?](#WhatIsDeepLearning)
	* [Conclusion](#Conclusion)
* [Machine Learning Algorithms](/tutorials/ai4bl/machine-learning-algorithms)
	* [Introduction](#Introduction)
	* [Machine Learning Algorithms](#MachineLearningAlgorithms)
* [Neural Networks](/tutorials/ai4bl/neural-networks)
	* [Introduction](#Introduction)
	* [Types Of The Neural Networks](#TypesOfTheNeuralNetworks)
	* [Summary](#Summary)
* [Why AI is taking off](/tutorials/ai4bl/why-ai-is-taking-off)
	* [Introduction](#Introduction)
	* [AI Is Only Just Starting](#AIIsOnlyJustStarting)
	* [AI Deployments Will Continue To Accelerate](#AIDeploymentsWillContinueToAccelerate)
* [How AI can benefit your organization](/tutorials/ai4bl/how-ai-can-benefit-your-organization)
	* [Introduction](#Introduction)
	* [Customer Experience And Service](#CustomerExperienceAndService) 
	* [Business Process Automation](#BusinessProcessAutomation)
	* [Cost Reductions And Operational Efficiency](#CostReductionsAndOperationalEfficiency)
	* [Data Security And Fraud](#DataSecurityAndFraud)
	* [Predictive Analytics](#PredictiveAnalytics)
	* [Staff Training](#StaffTraining)
	* [Summary](#Summary)
	
#### Who is who in AI

* [The Godfathers of AI](/tutorials/ai4bl/the-godfathers-of-ai)
	* [Introduction](#Introduction)
	* [Andrew Ng](#AndrewNg)
	* [Geoffrey Hinton](#GeoffreyHinton)
	* [Yann LeCun](#YannLeCun)
	* [Ian Goodfellow](#IanGoodfellow)
	* [Fei-Fei Li](#FeiFeiLi)
* [Big Companies and AI](/tutorials/ai4bl/big-companies-and-ai)
	* [Introduction](#Introduction)
	* [Google](#Google)
	* [Amazon](#Amazon)
	* [Facebook](#Facebook)
	* [Apple](#Apple)
* [The AI Startups Scene](/tutorials/ai4bl/the-ai-startups-scene)
	* [Introduction](#Introduction)
	* [The startup scene](#TheStartupScene)
	* [What are startups focussing on](#WhatAreStartupsFocussingOn)
	* [The most successful AI startups](#TheMostSuccessfulAIStartups)
	* [Summary](#Summary)
* [Autonomous Drive and AI](/tutorials/ai4bl/autonomous-drive-and-ai)
	* [Introduction](#Introduction)
	* [Waymo](#Waymo)
	* [Tesla ](#Tesla)
	* [Baidu Apollo](#BaiduApollo)
	* [Where next with autonomous vehicles?](#WhereNextWithAutonomousVehicles)

#### AI Use Cases

* [Computer Vision](/tutorials/ai4bl/computer-vision)

#### How AI is changing industry

* [Using Data and AI in Telecoms](/tutorials/ai4bl/ai-telecoms)
* [Using AI in Media](/tutorials/ai4bl/ai-media)
* [Using AI in Healthcare](/tutorials/ai4bl/ai-healthcare)
* [AI in Agriculture](/tutorials/ai4bl/ai-agriculture)
* [AI in Manufacturing](/tutorials/ai4bl/ai-manufacturing)
* [AI in Retail](/tutorials/ai4bl/ai-retail)
* [AI in Transportation](/tutorials/ai4bl/ai-Transportation)
* [AI in Finance](/tutorials/ai4bl/ai-finance)
* [How AI is changing the travel industry](/tutorials/ai4bl/ai-travel)
* [How can IoT AI assist in improving life quality for people with special needs](/tutorials/ai4bl/ai-iot) 


## <a id="WhatIsArtificialIntelligence"></a>What is AI

###  <a id="WhatIsArtificialIntelligence"></a>What is Artificial Intelligence 

When people talk about artificial intelligence (AI), their mind often goes straight to the movies. 
They seem to think that some company is going to create Skynet and manufacture Terminators to initiate the extinction of humanity. 
This is a common misperception of AI whereby movies have associated it with sci-fi and fantasy but in reality, it is quite different. 


AI is the use of computer science and programming that trains machine to imitate human tasks and thought processes. 
It works by analysing data and surroundings to solve problems, incrementally learning for itself to continually improve. 
AI functions are initially based or trained using the instructions given to it by humans and when these are a bit vague or incomplete, 
in theory we could get Skynet type consequences (albeit not quite so extreme).


Right now, all forms of AI use some sort of human intervention. That could be loading the training data or analysing the results and perfecting them. 
AI is not at a point yet where it has its own conscious decision-making process and can see the world as or better than a human would. 
This is likely to still be quite a long way in the future and it is important not to exaggerate the capabilities. 


Instead of talking about AI, discussing its applications helps to make better sense of the term and show how it is impacted many parts of everyday life.
 

###  <a id="ApplicationsOfAI"></a>Applications of AI 

You are probably exposed to AI every day. Whether it be using Facebook Messenger, talking to Alexa, watching Netflix, listening to Spotify or 
searching on Google, you are using a form of AI. Most of these examples are powered by an AI application known as machine learning. 


Machine learning is the use of existing data to make future decisions. Algorithms built without programming platforms are designed to enable 
machines to make unsupervised choices based on the data they have been supplied with. 
One of the best ways to explain machine learning is when comparing to how a baby learns to walk. 


A baby would start by taking in the surroundings and watching other children or adults walking. Nobody explicitly tells a baby to move their left foot 
forward, then the right, then the left again, then the right and so on. 
In gathering data from the environment, a baby will learn for themselves and attempt their first steps. 
Initially, they might fail so next time they use a table to help them up. Over time, the baby connects all the dots provided by data and begins to walk.

 
A machine will learn in the same way. Let’s say you want the machine to separate pictures of cats from pictures of dogs. 
To start, you give it a large collection of cat photos and it looks through to find the patterns. 
When it is presented with a new photo, it tries to work out whether it is a cat or dog. Every time the machine fails, it learns from the mistake and 
becomes more accurate. It can do this against vast volumes of data. In theory, the machine should become 100% accurate with the task.

 
Machine learning works using data so a human never has to program it. It might even find patterns that a human never would have done. 
A real-world example in Hong Kong has shown that machine learning has become more accurate than doctors in cancer diagnosis through analysing i
mages of patients who have symptoms. 


The digital world is full of data meaning machine learning has a major part to play. One of the key applications today is in conversational
chatbots which use a form of machine learning known as natural language processing. 
Amazon Alexa can take voice commands and analyse them against a huge knowledge base to return the most relevant response to the user. 
Sensors on factory machines are being used to constantly record data and predict when maintenance could be needed before any problems arise. 
In contract law, algorithms can review thousands of articles simultaneously and potentially solve cases in a split second that would normally 
take a human weeks or months. 
 
 
Those are just some applications, but it exemplifies the value of treating data as a business asset. 
AI is more about data than it is about the fantasy we see in the movies.



###  <a id="CompaniesThatUseAI"></a>Companies that use AI 

AI applications are used in almost every company that we interact with. Here are a few popular examples.

-	Google – every Google search uses machine learning. 
			 It takes what the user writes or says and applies that to algorithms, returning the most relevant results. 
			 Google can even do this with video now as the AI has advanced over several iterations since 2010. 

-	Netflix – the streaming service users what we would call a recommender system. Instead of users choosing a show to watch, 
			  Netflix uses data to predict what their subscribers will want to watch next. 
			  Recent statistics have suggested as much as 80% of user choices have come via the recommendations. 
			  Subscribers are even presented with different show thumbnails based upon their likely preferences. 
			  Spotify and Amazon use similar models

-	Facebook – the social network is massively based on data. 
			   Users get ads based on their preferences and it is never just a coincidence when they are relevant. 
			   Messenger is now a conversational chatbot used by major companies to complete retail actions without any human involvement. 
			   Facebook has utilised the purchase of WhatsApp to get a great understanding of how people converse. 

###  <a id="TheFutureProspectOfAI"></a>The future prospect of AI 

At the moment, we are only really at the start of an AI revolution. 
Applications like machine learning are still relatively new outside the big enterprises and have only been deployed as a very light touch. 
However, that said, many of the applications seem so normal that we don’t even remember they exist. 
Talking to Alexa to turn on your lights has become standard in some households. 
In the future, the same will most likely happen with driverless cars and robotics but we are a little way off that yet.   

What is becoming quite scary is that humans are beginning to trust AI applications more than another human. 
In fact, even when in a retail store, over 60% of people surveyed said they would rather use their SmartPhone to answer questions than ask a human a
ssistant. As new technology like driverless cars come into play, this lack of trust could start a societal breakdown of sorts which is perhaps 
why we are holding back just a bit with such game changers. 

## <a id="HistoryofAI"></a>History of AI

###  <a id="hoai-Introduction"></a>Introduction

We tend to see artificial intelligence (AI) as a brand new development but the history books would tell us otherwise. 
No longer the domain of science fiction, robotics and artificial intelligence and becoming important business drivers. 
This article looks at how we got to where we are today. 


The timeline below from the University of Queensland gives a brief overview of how AI has progressed over the years into becoming a standard 
part of university offerings. 

Whilst this timeline provides us wit a great overview, we are going to start back at 1921 at a time when the term “robot” was first used.


###  <a id="TheRiseOfRoboticsAndAI"></a>The rise of robotics and AI

The term robot was first used by Czech writer Karel Capek almost 100 years ago in 1921, although he credited his brother Josef Capek as being the 
inventor of the word. It comes from the word robota which is associated with labor or work in Slovak and gives us an insight into its intention. 


In 1939, a humanoid robot named Elektro was presented at the World Fair, smoking cigarettes and blowing up balloons for the audience. 
This was a couple of years before Isaac Asimov formulated his “three laws of robotics” that most of us will be familiar with from movies 
like “I, Robot.” The three laws of robotics state:

1.	A robot may not injure a human being or through inaction allow a human being to be harmed

2.	A robot must obey orders given to it by human beings except where such orders would conflict with the first law

3.	A robot must protect its own existence as long as such protection does not conflict with the first or second law

These laws still stand firm today and in their own way are built into artificially intelligent devices. 


Moving into the 1940s and 1950s and the foundations of neural networks in machines start to be developed.  
In many papers, this period is considered to be the true start of AI as computer science started being used to solve real world problems, 
moving away from just theory and fantasy. 

During the Second World War, the British computer scientist worked to crack the “Enigma” code which was used by German forces to send secure messages. 
This was done by Turing and his team using the Bombe machine and laid down the foundations for the application of machine learning; 
using data to imitate human tasks. 

Turing was amongst the first to consider that a machine could converse with a human, without the human knowing it was a machine. 
Many know this as the “imitation game”, another that has since been made into a popular movie. 

The standard was set for AI and in the 50s and 60s, research into the domain began to boom. 
In 1951, Marvin Minsky built the first neurocomputer and a machine known as Ferranti Mark 1 successfully used an algorithm to master checkers. 
At a similar time, John McCarthy who is often penned at the father of AI, developed the LISP programming language which has become very important 
in machine learning.


In the 1960s, there was more exploration around robotics as GM installed the Unimate robot to lift and stack hot pieces of metal. 
Frank Rosenblatt also constructed the Mark I Perception computer which was able to learn skills by trial and error. 
By 1968, a mobile robot known as “Shakey” is introduced and controlled by a computer the size of a room. 


###  <a id="TheAIWinters"></a>The AI Winters

Despite all this progress, during the 1970s, AI hit a period known as the AI Winters, coined as an analogy of the nuclear winter. 
Scientists were finding it very difficult to create truly intelligent machines as there simply wasn’t enough data to do so. 
This led to a slide in government funding as confidence began to dwindle. 


Research slowed until the 1990s apart from a few notable projects such a SCARA, a robotic arm invented for assembly lines in 1979 and 
research by Doug Lanat and his team in the 80s which looked to codify what we call human common sense. 
Also, in 1988, the first version of a conversational chatbot was launched and we saw a service bot in hospitals for the first time. 
Some of these developments created a bit of a spark and a renewed interest in the potential of AI going into the 1990s. 



###  <a id="NewOpportunitiesForAI"></a>New Opportunities for AI

During the 90s and going into the new Millennium, companies started showing a new interest in AI and it had a second coming of sorts. 
The Japanese government announced plans to develop a new generation computer to advance machine learning. 
In 1997, IBM’s Deep Blue computer famously defeated world chess champion, Gary Kasparov and really propelled AI into the limelight. 


Improvements in computer hardware meant companies had more data and therefore, greater opportunity to develop machine learning propositions. 
In 1999, although we were a way off seeing the likes of Pokemon Go, augmented reality was first coined as a framework and term. 
It seems as if these theories started to drive a wave of developments in the early 2000s as the Big 4 (Google, Amazon, Facebook and Apple) 
gained a major share of the AI market. 


In 2005, autonomous cars took a huge leave, driving 183 miles without intervention. 
IBM introduces their Watson AI assistant in 2006 which later defeated a Jeopardy champion in the US. 
Google launched street view in 2010 and in 2011 we met Siri for the first time. 
It wasn’t until 2015 that Alexa finally hit the market place and then drone deliveries started, Google lens became a reality, 
smart homes were the in thing and people started having their own Virtual Reality platforms at home.


In fact, for all the history of AI, the last ten years have been huge, creating so much data and allowing us to change life as we know. 
It is difficult to imagine life without AI in the modern digital world. This is quite an amazing feat when we considered the tumultuous times of the 
last century.

## <a id="FutureofAI)"></a>Future of the AI

###  <a id="foai-Introduction"></a>Introduction 

Artificial Intelligence (AI) is not a new innovation, although it certainly feels like it. 
Its origins are usually dated back to the 1920s when the term “robot” was first coined but it came into prominence during the Second World War 
thanks to Turing and the cracking of the Enigma code.  
Between the 1970s and the new Millennium, it struggled to gain traction as there simply wasn’t enough data for it to develop meaning nobody was 
willing to invest. However, the 21st century has seen a huge resurgence and since 2010, AI has dramatically changed everyday life. 

If we think about some of the things we do today, many didn’t even exist 10 or 15 years ago. 
We use Smartphones, check our social media, search using Google, talk to Alexa, stream on Netflix, listen to music on Spotify, 
buy products from Amazon, turn our lights from an App, catch Pokemon on the streets, play virtual reality games from our home, 
get an Uber or order food from Deliveroo. All these things are standard and show how far we have come in a short space of time. 
Every single one of them has a foundation of data and artificial intelligence. 

Whilst AI has brought us these amazing innovations, they are not that exciting anymore. If we expect Alexa to respond to voice commands, 
the fact that the technology is starting to do that better is great but not really a new development. 
A lot of the technology we have talked about falls into a machine learning application, ones that use data to make decisions. 
Google returns webpages that it predicts are most likely to be relevant and Netflix uses data to give us the shows it predicts we will like. 
This idea has become an expectation rather than an innovation. So, what’s next?


###  <a id="TheFutureOfAIBeyondExpectation"></a>The future of AI – beyond expectation

Experts in the field are primarily focusing on artificial general intelligence (AGI). 
Am AGI machine is one which can perform any cognitive task that a human can. Whilst the technology we have is amazing and useful, it is not cognitive, 
it does not have a concept of the world or a conscious mind if you will. 
Existing AI would not be able to pass the Turing test in which a computer must prove its intelligence as indistinguishable from that of a human. 
That is the goal for AGI and the future aims to move us closer to that point. 
Many believe that could be in the next 5 to 10 years whereas other would suggest 20 to 30 years is more accurate. 

A machine with AGI would be able to perform any task that a human being can do without being programmed to do so. 
For example, if asks to hammer a nail it would simply start guessing at a few things and continue to fail until it got it right. 
The basis of AGI would be trial and error, the same as when a human learns to walk for example. 
We should probably remember that scientists don’t even fully understand the human brain yet, let alone training a machine to do the same.


###  <a id="ComputingPower"></a>Computing Power 

One of the limitations to further development has been the lack of computer power available. 
Some of the aforementioned slow periods have also been caused by a lack of data to work with. 
For AI to get close to the intelligence of a human brain, we need what is known as quantum computing. 
Whilst the technology has been released by the likes of IBM, it is still very much in its infancy and we don’t know if they will become 
mainstream quickly. 

What this means is that whilst AI might be able to look at images of cars and build the models, it is some way off being creative enough to 
come up with its own ideas for building a car like a human would. 

###  <a id="EmergingTechnology"></a>Emerging Technology

Whilst AGI might not be imminent, advances in cloud technology, edge computing and Big Data platforms should bridge the gap between AI and robotics. 
These technologies are helping to process data faster and more effectively meaning robots can make better decisions and become more useful. 
For example, AI powered robots can carry out dangerous tasks safe in the knowledge they know what they are doing through existing datasets. 
Machine learning won’t just power Google, Facebook and Netflix but also more practical events that make our lives safer. 

###  <a id="AssistantsWillBecomePredictive"></a>Assistants will become predictive


As it stands, Alexa and Google Home respond to the commands we give them. What if they were able to start predicting what we needed? 
The same applies to other smart home devices. For example, a smart refrigerator can work out when you run out of milk and order some to be 
delivered to your day without any intervention. 
Instead of asking Alexa to turn off the lights, smarter assistants will know when you need events to happen. 
As assistants gather data, they are becoming intelligent enough to be predictive.

###  <a id="AffectiveComputing"></a>Affective Computing 


Emotional intelligence is something that AI traditionally lacks. If you say the same thing to Alexa in a happy tone or a sad tone, 
you receive the same response. Affective computing is a field that studies speech or body language and images to recognize our needs. 
For example, conversational chatbots can respond differently based on how we speak or type. 
This is done by monitoring pauses on speech or pitch of voice as a measure of emotion. The aim is to start making devices appear more human. 

###  <a id="ChangingProfessions"></a>Changing Professions 

Given the fact that AI can diagnose medical conditions through image scanning and data we will start to see changes in healthcare. 
Doctors will be able to care for patients rather than spending time on diagnosing them because AI can carry out the task in a split second. 
AI can also review contracts, provide measurements for construction or pay out insurance claims. 
Professional interactions will slowly evolve over the next few years. 

###  <a id="LetsGetDigital"></a>Let’s get digital 

Let’s get digital
Everything will be digital. Technology such as Blockchain can be used to verify our identity so we don’t need to carry ID cards. 
The movement towards cryptocurrency like Bitcoin will continue. 
Ultimately, we will interact digitally making everyday life far more efficient than it is today. 

# Understand AI

## AI, Machine Learning and Deep Learning

###  <a id="Introduction"></a>Introduction

Although Artificial Intelligence (AI) has been around for some time, it is still common to get caught up in the buzz without truly understanding 
what it means. People think that AI is a robot that can do things a smart person would, knowing everything and being able to answer every question. 
This is what television and movies have led us to believe. Creating these ‘conscious’ machines is the goal of researchers and professionals but we are 
not close to that yet. 


AI is classified into two groups. General AI is the concept explained above where machines can intelligently solve problems without human input. 
A machine with general AI capabilities would have cognitive abilities and interpret the environment around it. 
It would be able to process this information far quicker than any human could leading to these sci-fi ideas of superior beings. 
General AI currently is beyond our reach but as the volume of data in the world grows and computing power increases, we will get closer. 


In the here and now, we are in a time of what is called narrow AI. A machine with narrow AI capabilities is one that operates from a predefined 
set of rules. This could be a Netflix recommendation engine or a voice command system like Alexa. Both are examples of artificial i
ntelligence but are fed from criteria or training data to function.   
A good example is a driverless car which although impressive, is still narrow AI as it is given a set of rules to operate by. 
Until a car can understand the environment and think for itself, this will always be the case.

Narrow AI applications are driven by two subsets of AI known as machine learning and deep learning. 
The best way of explaining the link between the three is that AI is an all-encompassing term, inside of which is machine learning and then 
within that we get more complex deep learning. 


###  <a id="WhatIsMachineLearning"></a>What Is Machine Learning ?

Machine learning is often described as a method for realising AI. A computer or machine is loaded with vast amounts of data which it will use to 
train itself. The data might be labelled initially to make things easier. 
For example, if we want a machine to recognise photos of cats, we may load it with thousands of images of cats and dogs, labelling them appropriately. 
The machine will take a new image and find the label it matches to learn whether it is a cat or not. 

Machine learning is the process of enabling machines to learn through data. The predictions the machine makes from that data is what we know as AI. 
If we go back to the example of Alexa. Alexa receives a voice command, interprets that using an algorithm (known as natural language processing or NLP),
matches the result against all existing data stored in the cloud to find the appropriate response and sends that back as a reply. 
Alexa gives the impression of being a cognitive machine but is far from it.

There are four common machine learning methods.

1.	Supervised learning

This method takes existing data and trains a model to work out how to classify a new piece of data. 
For example, it could hold data on the symptoms of diabetes and when it receives blood test results of a new patient, it is able to diagnose 
accurately from the data. It will classify the patient as having diabetes or not having diabetes.

2.	Unsupervised learning

Unlike, supervised learning, these models will attempt to classify data without any prior knowledge. 
The algorithms look to find patterns themselves and put data into groups. A common example is something like customer purchasing behaviours. 
The algorithm won’t have existing labels and will decide on its own how to classify the data, often known as clustering. 
Imagine going to a party where everybody is a stranger. Your mind will probably classify people based on age, gender or clothing. 
You don’t know them but have still worked out the classifications. 

3.	Semi-supervised learning

As title suggests, this is a mix between supervised and unsupervised learning. 
In our data, some items are labelled but some are not. 
Where you have vast amounts of data this can be quite common. 
A semi-supervised model would have some labelled data to know that classification does exist. 
It is then trained on unsupervised data to define the boundaries of what it is looking at and potentially specify new classifications that the 
human did not specify when labelling.

4.	Reinforced learning

This application is about positive and negative rewards for certain behaviours. This will be a common method in robotics where machines learn to 
optimise behaviour from experiencing positive or negative results. 
For example, if a robot found a TV remote and decided to throw it, it would break and be a negative result. 
However, pressing a button turns the TV on a produces a positive result so it continues to do it. 
The robot will continue this process until finding the best possible result.

Whilst every AI-based project is unique as they all run from different datasets and rules, there are key algorithms that you will find in the 
library of virtually every Data Scientist.  


###  <a id="WhatIsDeepLearning"></a>What Is Deep Learning ?

Deep learning is a subset of machine learning. In a sense, it is an evolved version of machine learning methods. 
It is inspired by processing patterns of the human brain known as neural networks. Whereas machine learning techniques will take an input of data 
and learn from it, deep learning neural networks learn through their own data processing. 

Unlike in machine learning where even in unsupervised methods a human still jumps in if the model gets too confused, deep learning algorithms 
decide for themselves whether a prediction is accurate. 

It is difficult to ensure deep learning neural networks don’t come up with incorrect conclusions but when it works, it can get us a step closer to 
general AI. 

AlphaGo by Google is one of the most popular cases of machine learning. Google trained a machine to learn the board game Go which requires a 
lot of intellect. Without being told what move to make, the machine learnt the rules itself and began to outperform humans. 
If the computer had of been fed rules through machine learning, this may not be hugely impressive but the fact it learnt how to win on its 
own is incredible. 

There are many types of neural network that are used for different applications. 


###  <a id="InConclusion"></a>In Conclusion

Narrow AI can be seen everywhere from GPS systems to Alexa and recommendation platforms. 
Machine learning and deep learning have benefitted from large investments at the start of the 21st century as consumers seek ways to 
become more efficient and have an easier life. 

However, the ultimate goal is artificial general intelligence, a self-teaching system that can outperform humans across a wide range of disciplines. 
It is thought that this could be 30 years away or some say as long as a century but as computing power and data evolve, we will continue 
to see more amazing developments.

## Machine Learning Algorithms

###  <a id="Introduction"></a>Introduction

In our previous article, we gave an overview of the difference between artificial intelligence (AI) and its subsets known as machine learning (ML) and 
deep learning (DL). 

Both ML and DL have a wide range of uses across several industries and whilst the applications might be different, 
the algorithms and neural networks tend to have the same foundations. 

This article talks about some of the most popular technical algorithms used in machine learning and what they do as well as examples of 
neural networks used for deep learning.


###  <a id="MachineLearningAlgorithms"></a>Machine Learning Algorithms

When you start trying to learn Data Science and begin to research programming platforms like R and Python it can be intimidating. A lot of the techniques come with multiple page definitions and descriptions and it is very difficult to put the detail into a practical use case. If you dream of becoming a true expert and earning the big salaries, you will definitely need to have a handle of the ins and outs of everything but as a starter, this brief guide attempts to define a realistic beginning.
https://blog.usejournal.com/machine-learning-algorithms-use-cases-72646df1245f

1.	Linear Regression

This is one of the quickest algorithms for a machine learning beginner to master. 
Essentially, if we have a set of variables (x) that are used to determine an output variable (y), the goal is to quantify the relationship 
between the two. 
This would be used in something like sales forecasting or risk assessment. 
It will show you what happens to dependent variables i.e. sales when changes are made to independent variables

2.	K-means Clustering

Used in applications such as grouping images together or detecting activity types in motion sensors as well as structured data use 
cases like customer segmentation. This unsupervised learning algorithm takes unstructured data and separates it into ‘K’ groups. 
It will classify the data and categorise it based on specific features. 

3.	Logistic Regression

Predictions based upon continuous values after applying a transformation function. 
Unlike linear regression, the output will be the likelihood of an event occurring rather than a precise number like a sales figure. 
This could be whether a student will pass a test or if an employee is likely to be sick for example.

4.	Support Vector Machine (SVM)

This is a classification algorithm used for category assignment like detecting spam emails and sentiment analysis projects. 
It is a form of supervised learning that looks for support vectors that are along what is known as a ‘hyperplane’, the line that separates 
and classifies a set of data. It is designed for smaller datasets and is often more efficient than other algorithms given that it users a subset of 
training points. 

5.	Decision Trees

A supervised learning method used in classification type problems. A decision tree is probably best explained using an example. 
Imagine we have 30 students with boy/girl, height and class variables. 15 out of 30 of them play soccer in their spare time. 
A decision tree will segregate the students based on values in the three variables and identify which create the best homogenous set of students. 

6.	Naive Bayes

A probability algorithm that outputs the chance of an event occurring given that another event has already occurred. 
For example, if a student fails one test, what is the probability of them passing or failing their next test. 
It assumes all variables are independent of each other, hence the term ‘naïve.’

7.	Random Forest

Taking decision trees to the next level, a random forest algorithm constructs a number of tress together. 
The output will take a majority vote from the trees, so to speak, or take the average if the trees are producing numerical values.  

8.	Principal Component Analysis (PCA)

This algorithm is used in applications such as stock market prediction and pattern classification tasks. 
A principal component analysis tries to identify patterns in data and make correlations of the variables within it. 

9.	K-Nearest Neighbour

As the term neighbour suggests, this algorithm looks for similar items in comparison to others. 
It works well with unstructured data like images where the algorithm needs to have a “best guess” in some cases about what the 
likely output or classification should be of some input. 
It may no be accurate at first but can become very powerful, take Amazon Alexa as an example. 

10.	Recommender System

This algorithm filters and predicts user ratings and preference by using collaborative and content-based techniques. 
The most popular examples of how this is used in the real-world are Netflix, Spotify and Amazon. 
In essence, it makes recommendations based on how different pieces of data have been classified e.g. genre in Netflix. 

## Neural Networks

###  <a id="Introduction"></a>Introduction

Whilst neural networks are not as complex as somebody starting out in Data Science might think, it would be wrong to say they are simple to learn. 
These deep learning networks will transform data until it can be classified into an output. 
It consists of neurons (for information processing, like the brain) which multiply an initial value by some weighting, 
works out a bias as new values come in and then normalises the output with a function. 
It is a bit like how our brain would try to develop concepts and learn about the environment but instead, based on mathematical 
functions and programming. 

###  <a id="TypesOfTheNeuralNetworks"></a>Types Of The Neural Networks

 There are 6 types of neural networks used for different applications. The list below provides an overview of each. There is a lot more technical detail behind these available online and via publications to help work out which is best for your application. 
1.	Feedforward

In this neural network, the data or input travels in one direction which is why it is often known as the simplest. 
The applications tend toile in computer vision and speech recognition as where classifying the target classes are complicated. 
The feedforward network responds to noisy data and is quite easy to maintain. 

2.	Radial Basis

This type of neural network considers the distance of a point in relation to the center. 
They are used for time-series calculations and system control amongst other applications. 
A radial basis network will use a set of prototypes along with other training examples and find the distance between and input and a prototype. 
The activation functions of artificial neurons drive outputs that can be represented in different ways to show how the network classifies data points. 

3.	Multilayer Perceptron

Comprised of one or more layers of neutrons. Data is fed into an input layer and there may be one or more hidden layers providing 
levels of abstraction and predictions are made on the output layer which is also known as the visible layer. 
This is suitable for classification prediction problems where inputs can be assigned to a class or label. 
Data is commonly provided in tabular format like a CSV or Excel sheet. 

4.	Convolutional

Used for image classification, object detection and image segmentation. Networks have convolutional layers that act as hierarchical object extractors. 

5.	Recurrent

This type of neural network models sequences by applying the same set of weights recursively to the state of the aggregation at time (t) 
and input at time (t). 
The neural network is used for text classification texts, machine translation and language modeling. 

6.	Modular

A modular network is one composed of more than one neural network, connected by some intermediary. They can allow for more sophisticated use of basic neural network systems managed and handled in conjunction. Each individual network within the model should hope to accomplish some subtask of the wider objective. 


###  <a id="Summary"></a>Summary

We have provided a brief overview of popular machine learning algorithms and types of neural network. 
The technology and scripting behind them is of course far more technical than can be explained in a short article and does require previous 
experience in mathematics and data science fields. 

However, for anybody beginning an AI journey, knowing about how these models function is important and it is worth getting to know them in more detail.
  
## Why AI is taking off

###  <a id="Introduction"></a>Introduction

The term Artificial Intelligence (AI) has been around for a long time. 
It was coined by John McCarthy in the 1950s and he is considered one of the founding fathers of AI alongside Marvin Minsky.  
As well as that, some of the key applications like machine learning, deep learning and neural networks have been widely researched and deployed 
during the 70s, 80s and 90s. 
However, during each of those periods, the industry suffered from what we call “AI Winters” where initiatives didn’t have enough investment to succeed.
 
In the 2000s, big companies like Google, Facebook and Baidu arrived on the scene and started putting big money into research again, 
realising the potential of the solutions. 
Whilst AI is flying right now, what’s to say that we won’t have another “Winter” up ahead? 

How do we know that AI has really taken off this time? 


###  <a id="AIIsOnlyJustStarting"></a>AI Is Only Just Starting

The big difference between the AI applications of the 2000s and those of the past is commercialisation. 
Whilst the Turing Test was revolutionary in 1950 and machines winning at checkers, chess and Jeopardy were impressive developments, 
they didn’t learn to any real world, practical application of AI in a commercial sense. 

There are three key elements as to why AI has been able to become so powerful in the last decade. 

1.	Computing Power

Looking back to the 1950s, the early computers didn’t have sufficient power to create truly autonomous functioning systems. 
Some of the hardware and infrastructure might have been out there but it was incredibly costly where there were no real use cases that 
could prove the value of any investment. 

Imagine trying to run Alexa on a dial-up internet connection. It simply would not have been possible. 
Today, people will abandon a webpage if it hasn’t loaded within 3 seconds. 
That is the extent to which computing power has changed consumer expectations. Only in the last few decades has computer processing power been 
enough to support an AI system.

IBM is now working towards developing even more powerful quantum computing platforms that will take 
AI to the next level (although full adoption is a little way in the future). 

2.	Big Data

With the hyperconnected digital world, we are creating more data now than ever before and it is still growing. In fact, by 2025, intelligence firm 
International Data Corporation (IDC) are forecasting we will have 10 times more data than we did in 2016. 

Big Data underpins most of what we are doing with AI. For example, social networks have unleashed a whole amalgam of data that never previously 
existing amount human behaviours. Amazon have collected enough shopping data from their consumers to now accurately predict what they are 
likely to purchase next. Netflix can tell their subscribers what they want to watch before they even know.

The fact is, as more of our lives become digitalised, the more data becomes available and the potential for AI applications can only increase. 

3.	Models and Algorithms

For Big Data to be effective, we need good algorithms. These are scripts that instruct AI technology what to do. In some of the earlier applications of AI, these algorithms were very prescriptive and told machines what to do on a step by step basis. They have now become so sophisticated that computers can build their own algorithms without supervision to an incredibly high degree of accuracy. 

Data and computational power has led to a rise in deep learning and neural networks with refined algorithms. For example, algorithms can now be modelled that were not possible 20 years ago where there wasn’t the volume, speed and accuracy of information to create commercial value. 

4.	Democratisation of AI

Data Science and AI resource is still hard to find and for that reason, it can be expensive. 
Data Scientists were in a position where they could virtually create their own salaries given their unique skillsets. 

A growing number of tools are ensuring AI capabilities can be put into the hands of non-technical experts. 
Large enterprises including Google, Microsoft and IBM and releasing cloud-based tools that allow almost anyone to create their own machine 
learning models. These tend to use pre-built algorithms that can be applied to various situations without the need for technical support. 

Companies like DataRobot allows uses to upload data and quickly try different algorithms to see which obtains the best results. 

BI platforms such as Tableau, Qlik and Sisense amongst countless others can analyse data in an instant without needing any specialist resource.

As AI becomes commonplace in business, all these attempts to democratise access to it will speed up adoption across numerous functions. 
 

###  <a id="AIDeploymentsWillContinueToAccelerate"></a>AI Deployments Will Continue To Accelerate


Whilst what we have seen from AI so far has greatly impacted everyday life as well as our jobs and the ways many industries work, 
we are only at the start. To date, what we have is “narrow AI.” 

These are applications that have programmed rules to adhere by and function but the goal for researchers and developers is to achieve 
an artificial general intelligence (AGI). 
This is where machines can truly imitate human actions in a conscious way such as understanding the environment around them. 
A robot may be able to walk and perform activities against set rules, but it is not yet capable of designing those rules for itself.
 
Narrow forms of AI are still in the early stages of adoption and have already started diagnosing disease, solving legal cases and educating pupils. 
With the data available, potential computing power and democratisation of systems, applications will become mainstream in the forthcoming years. 

Following that, we may be able to set our sights on AGI but we are some way off that kind of singularity just yet. 

## How AI can benefit your organization

###  <a id="Introduction"></a>Introduction

There is a huge buzz around artificial intelligence (AI) and its subsets such as machine learning and natural language processing. 
This hype has largely been created by the transformational impact it is having on businesses. 
The speed at which organisations can develop AI based technology and increase their value is key to having a competitive advantage. 
Whilst many organisations are still trying to understand exactly what AI is capable of, this article summarises some of the key 
benefits that are already being realised. 

###  <a id="CustomerExperienceAndService"></a>Customer Experience And Service

One of the most common forms of AI is the conversational chatbot. These are messaging apps, speech-based assistants or voice activated devices that 
are used to automate communication and create a very personalised customer experience. 
These Internet of Things (IoT) based applications can process vast amounts of data instantly meaning they can make faster and more 
accurate responses than a human would ever be able to. 

Similar personalisation that makes best use of data can be used in marketing. This is where we get emails that are relevant to us and social media 
ads that just happen to be something we are interested in. In some cases, each customer can even see different website homepages depending on their 
likely preferences and what will interest them the most. 

Utilising AI in these ways is a great way to ensure customer loyalty through a personalised experience. 


###  <a id="BusinessProcessAutomation"></a>Business Process Automation

Businesses that have been established for a long time tend to have several manual processes. 
AI is a natural partner to optimise these efforts given its efficiency at handling routine tasks, improving interfaces, 
willingness and speed to do monotonous tasks and ability to handle massive amounts of data. 

There are some obvious processes like using robotics in factories, managing conditions in product storage, processing payments and registering 
customer requests but these only touch the surface of the possibilities. 
Doctors can use AI devices to dictate clinical notes which automatically fills in the relevant forms and orders a prescription. 
Lawyers will use AI to process contracts and agreements in a split second that may have taken them days or weeks. 

Essentially, anything that can be turned into a digital format has the potential for automation. 


###  <a id="CostReductionsAndOperationalEfficiency"></a>Cost Reductions And Operational Efficiency

An improved customer experience and process automation will have a clear knock-on effect when it comes to reducing business costs. 

In the context of efficiency, where AI can process vast amount of data so accurately, it can greatly reduce the number of business errors, 
improve workflows to increase production outputs and free up employee time for higher-level tasks. 
As an example, in healthcare, unnecessary tests as estimated to cost $210 billion per year in the US alone. 
Even if AI reduces this by half it is doing an amazing job. 

Gartner predicts that 85% of service interactions will take place between human and AI in 2020. This reduces to cost of business call centres and 
allows businesses to focus their staff elsewhere. For example, they may be redeployed into jobs that support the AI and digital technology. 
More to the point, AI is never sick and doesn’t need a holiday! 

Whilst AI can be costly to deploy, in the long run the savings will heavily outweigh the investment. 


###  <a id="DataSecurityAndFraud"></a>Data Security And Fraud

AI can be used to help identify fraudulent transactions and prevent unauthorised access to data. 
In an exponentially growing digital world, this is especially important when it comes to defending cyber-attacks. 
Powerful algorithms can find malware and combat spam for example. 
Machine learning will detect irregular patterns in the data and inform businesses when there is a potential threat. 

As well as this we are seeing the increased utilisation of identity checks other than passwords such as facial recognition and fingerprint technology. 
These unique identifiers based on unstructured data are far more difficult to hack and offer a great layer of protection for businesses.  

###  <a id="PredictiveAnalytics"></a>Predictive Analytics

Predictive analytics could be called the heartbeat of AI. Traditionally, management information would report on what has happened in the 
business e.g. we sold 100 pairs of shoes yesterday. Machine learning algorithms will make decisions on what is going to happen in the future. 
It will do this by finding patterns in data and making decisions based on that. 
For example, it can predict when a customer is next likely to want to buy a pair of shoes and ensure you are at the front of the queue when they 
come to market. 

Another example comes in supply chain management where machine learning can predict when stock is likely to run out or whether there is going to be 
product surplus.  

###  <a id="StaffTraining"></a>Staff Training

AI is being used in businesses to create personalised training plans. Some companies could have huge knowledge bases that take staff weeks or even 
months to learn. AI has been shown to cut this in half by presenting content to the learner in the way that best suits them.
This could include the order they learn items in, the length of time between when learners are presented with repeat information or the 
type of material such as written, visual and audio. Training is both more useful and enjoyable. 

###  <a id="Summary"></a>Summary

Whilst many applications are still relatively immature, companies need to prepare for investment in AI if they are going to keep up with the 
competition. It should be made clear that AI is not going to replace humans but instead, it will provide the technology to run monotonous, 
tedious and repetitive jobs, allowing people to focus on what they are best at. For example, AI will help diagnose disease so doctors can 
concentrate on caring for patients. AI will deliver learning curriculums, so teachers can worry about helping the students.

To make the most of these powerful efficiencies, AI should be considered as a means of augmenting and not replacing human capabilities. 


# Who is who in AI

## The Godfathers of AI

###  <a id="Introduction"></a>Introduction

With so much press and hype surrounding artificial intelligence (AI) and its applications over recent years, it is very difficult to keep 
track of everything that is going on. 
Social media has a vast array of channels for professionals, reaching over may industries and focussing on differing levels of technical and 
strategic abilities. To stay connected, there are some experts and influencers that everybody with a vested interest in AI must ensure they follow. 
This article talks about some of those key players in the data-centric world around us. 


###  <a id="AndrewNg"></a>Andrew Ng

Considered as one of the top minds in machine learning, Andrew Ng is having a huge impact on AI education. 
He is best known for co-founding Coursera, the online education portal and his machine learning course at Stanford University remains the most 
popular on the site. 
In 2017, Ng started Deeplearning.ai, a project specialised in deep learning education projects aimed at engineering and mathematics students.
 
At the start of 2019, “AI for Everyone” was released on Coursera. Ng said on his blog that the AI-powered future must be built by both engineers 
and application domain experts. The course is designed to ensure we have experts in every industry who can apply AI to their organisations.
 
Credited as the founder of the Google Brain project and featured in the Times 100 Most Influential People, Andrew Ng is a go to source when it 
come to AI and machine learning. 
Since 2018 he also launched and currently heads the AI Fund, a $175 million investment fund for backing artificial intelligence start-ups. 


###  <a id="GeoffreyHinton"></a>Geoffrey Hinton

Hinton is a leading figure in the deep learning community and even been penned as the “Godfather of Deep Learning” by his peers. 
He was recently named as of the three recipients of the A.M. Turing Award for his decades of work advancing the field of artificial intelligence. 
The deep learning techniques inspired by Hinton and his collaborators have achieved significant breakthroughs in several applications from 
voice recognition in mobile devices all the way through to diagnosing cancerous tumours in medical scans.
 
Hinton has taught his own course via the Andrew Ng owned Coursera and worked for Google from 2013. 
The focus of his research is ways in which neural networks can be used for machine learning, memory and perception. 
He has put his name to many research papers and is the co-inventor of Boltzmann machines, one of the first neural networks capable of 
learning internal representations.

AI is in Hinton’s blood, being the great-great-grandson of logician George Boole, whose work formed the foundations of modern computer science. 


###  <a id="YannLeCun"></a>Yann LeCun

As the VP and Chief AI Scientist at Facebook, Yann LeCun has quite the list of credentials. 
He was part of the team that were awarded the Turing Award in 2018 alongside Geoffrey Hinton and is the founding Director of Facebook AI Research 
and the NYU Center for Data Science. 

LeCun is best known for his contributions to deep learning and neural networks with applications used in computer vision and speech recognition 
technology. With over 190 papers published in this area as well as several other AI topics, 
LeCun is high up the list of experts when it comes to the subject. 

Beyond his role at Facebook, LeCun has co-founded start-ups including Elements Inc and Museami. 
He has too many qualifications to mention as well as being in the New Jersey Inventor Hall of Fame and a member of the 
US National Academy of Engineering. 

Like Hinton, LeCun sees an amazing future in using AI to solve real world problems such as diagnosing diseases through medical image analysis. 


###  <a id="IanGoodfellow"></a>Ian Goodfellow

Sometimes referred to as “The GANFather,” Ian Goodfellow is the inventor of a powerful AI tool that can pit different neural networks against each 
other. GAN stands for a Generative Adversarial Network, an approach to machine learning frequently used at Facebook. 
Given a training set, the approach learns to generate new data with the same statistics as the training set. 
For example, a GAN trained with images can generate new images that look at least superficially authentic to observers. 

One use case could be in fashion where GANs can create photos of imaginary models with no need to hire a model, photographer or equipment. 
This could drive entire campaigns.

Yann LeCun has said that GANs are the “coolest idea in machine learning in the last twenty years.”

Goodfellow obtained his BS and MS in computer science under supervision from Andrew Ng. 
He joined Google as part of the Google Brain research team and after a brief stint away at the OpenAI Institute, re-joined Google in 2017. 


###  <a id="FeiFeiLi"></a>Fei-Fei Li

As a former Google employee and Professor at Stanford University, Fei-Fei Li is one of the most prominent women in the world of AI. 
She is well known for her non-profit work as the Co-Founder and Chairperson of the organisation AI4ALL whose mission is to educate the next 
generation of AI thinkers and leaders. 
In 2018, AI4ALL successfully launched five more summer programs in addition to that at Stanford University due to its overwhelming success.
 
Li has published around 200 papers on AI, machine learning, computer vision and neuroscience-based topics. 
She has been highly commended as a pioneer and researcher who strives to being “humanity to AI.” 
Amongst her portfolio is the ImageNet project which has revolutionised the field of visual recognition. 
Many see the work as a catalyst to the current AI boom. 


## Big Companies and AI

###  <a id="Introduction"></a>Introduction

Artificial intelligence (AI) is the new weapon that is starting to define how large companies compete with each other. 
The Big Four or GAFA (Google, Amazon, Facebook, Apple) companies have had an advantage of large budgets, talented resource pools and vast amounts of 
data to develop AI solutions at a faster rate than anyone else. 
This is creating a problem for smaller businesses who cannot compete with many of the offerings and get drowned by a GAFA tidal wave. 
This means small to mid-sized businesses need to find their own niches to remain competitive. 

Rather than focussing on the trials and tribulations of those small businesses, this article looks at GAFA and how they are progressing their AI 
offerings. 


###  <a id="Google"></a>Google

AI radiates through almost every that Google does. In fact, they have stated that they are moving towards an “AI first” development structure. 
It has been around 12 years since Google introduced us to their Android operating system, an open source platform for mobile devices. 
In April 2019, they announced they would be doing the same with AI, using TensorFlow, their open source platform for machine learning. 
Essentially, anyone who can connect to the internet has access to one of the most powerful AI platform ever created.

Google has virtually become an AI company, far removed from simply a search engine, and they are starting to introduce that to the rest of the world. 
Although there are other companies with platforms like TensorFlow, they don’t have the same development, research and funding power that Google has. 

Whilst in theory, providing these complex machine and deep learning tools to everyone with allow them to compete with Google, the objective is that 
it accelerates computer science and the community gives back. 
This is how Google see the future and they have set themselves up as a transformative AI company. 

Beyond TensorFlow, there are a vast amount of ways that Google already deploy AI applications. 
Below is a summarise of some of the most well-known applications. 

-	Google Search

Most of us would know that the Google Search algorithms are powered by AI, or more specifically machine learning and natural language processing. 
However, this has come a long way over the years, especially with the prominence of voice search and deep learning techniques are allowing the 
models to learn on their own. 

-	Adwords

Google Adwords now uses AI to enable Smart Bidding. Instead of employing a digital marketing team to manually arrange auction bids, machine 
learning algorithms will automate the process to improve conversions whilst reducing the cost of labour. 
It is able to user a wider range of contextual signals within the machine learning models that may not have been picked up otherwise. 

-	Maps

Consumers are now able to use Google Maps just like a satellite navigation system. It has effectively replaced the need old-fashioned 
GPS monitor (TomTom) and allowed one less device in the car. The integrated AI models can flag driver alerts and link to place of interest. 
It also detects traffic delays and route problems for drivers. The predictive nature of Google Maps means you can navigate without any commands.

-	YouTube

Owned by Google, one of the core uses of machine learning at YouTube is ensuring that brands don’t have their ads placed next to what might be 
deemed as inappropriate content. Users are also fed recommended videos in a form of predictive AI. 

-	Photos

Although not one of the most used features, Photos will recommend images you should share and which friends you should be sharing them with. 

-	Gmail

The email platform from Google has really stepped up the AI game in the last year. Platform users now have access to smart replies. 
The platform will offer up predictive sentences as you start writing content based on the data within your inbox. As it gathers more data, 
it learns your writing and it able to accurately create an email for the user. 

-	Calendar

Within Drive is Smart Scheduling which can suggest meetings and appointments based on the regular habits of the user. 

-	Drive

The AI used within Google Drive and Documents is able to predict which files you are looking for and claims to reduce the time spent finding it 
by up to 50%. It will display the files it thinks you need at the top of the screen each time. In Google Sheets, 
it can now even automatically generate formulas whilst Documents use natural language processing for better functionality.

-	Assistant

Google Assistant, the voice activated agent, gets answers almost instantly straight from the web, pretty much as if you were doing a search yourself. 
Google Assistant is also great at remembering previous conversations and syncing to Maps and other Google products. 
The assistant is the power behind the Google Home IoT device which has laid down its marker in the voice command device market. 
Google Home has been learning at an accerlerated rate thanks to the massive amount of data that the company hold.

-	Allo

This is the Google venture into the messaging App world. It goes beyond some of the other apps through deep learning neural networks which 
Google say will be powerful enough to create its own emojis, rather than the standard ones that come with other platforms. 
Everything Google is doing has an AI first thought process.


###  <a id="Amazon"></a>Amazon

Amazon has not become the retail giant that it is today by accident. 
AI is shaping everything they do from the warehouse right through to the Alexa smart speakers. 

-	Product recommendations

Virtually from the start, Amazon has used machine learning algorithms to recommend products to consumers. 
The predictive AI will present users with products they are most likely to want based on either their purchasing behaviour or the actions of other 
consumers just like them.

The correlations for the recommendations have gone though a number of changes over the years to account for fast-paced and dynamic markets. 
For example, if a new product line comes to market, it can tell instantly which consumers are going to be interested in it. In the earlier days of 
the machine learning algorithms this may have taken some time to work itself out. 

Reports have suggested that as much 35% of Amazon’s retail revenue comes from products that are purchased via its recommendation engine. 
To put that into context, AI is able to sell products that a consumer would not have made a conscious decision to buy otherwise. 
If you add personalised emails into the mix and the excellent on-site search, Amazon is driving much of its retail sales using data alone.

-	Voice control devices

Unless you have been hibernating for the last few years, you will have noticed the amazing rate at which Alexa has come to the market. 
Millions of consumers have now bought Alexa and there are almost 50,000 skills available on the device. 
This means they can help with a wide variety of tasks from simple voice commands.

We should not underestimate the amount of data in the Alexa engine and the immense processing power that can send answers back almost instantly. 
There are now integrations with many other devices, capitalising on the dominance of the market. 
They are creating customisable skills for third parties e.g. Marriott Hotels as a type of concierge system for guests. 

The future of Alexa is in letting consumers create their own skills via their Blueprint platform which doesn’t require any development knowledge. 

-	Amazon Web Services (AWS)

AWS is the massive cloud based service and storage facility provided by Amazon. It has become one of the market leaders in this sector and is a 
huge contributor towards the trillion dollar valuation of the company. 

The aim of AWS is to give users the same capabilities that Amazon has in creating its machine learning models for recommendations, Alexa, DeepLens or 
Amazon-Go (see below). AWS is tailored for different levels of users, from beginners all the way to those with Data Science or equivalent type degrees. 

Usage of AWS grew by 250% during 2018 and is set to continue as the cloud platform of choice for many businesses.

-	The Warehouse

Albeit not consumer facing, one of the most important areas for AI is in the amazon warehouse. 
The facilities are filled with robotics that spur into action as soon as somebody places an order. 
They are completely autonomous and will deliver items to a human who checks it and puts it on a conveyor belt. 
With the volume of orders received, every second counts in the warehouse which is why AI is vital for improving these efficiencies.

As well as this, Amazon use machine learning to predict what customers are likely to be ordering and put it in the right spot of the warehouse, 
improving speed of processing. Using computer vision, Amazon have also optimised the scanning processes for goods in the warehouse with a 
new system saving workers huge amounts of time. 

Delivering items to the customer on time is imperative to the success of the Amazon offering so every second saved is another happy buyer.

-	Amazon-Go

Trials of the Amazon-Go stores showed customers walking in, picking up groceries and walking back out again. 
Sensors and scanners can tell exactly what they have picked up and charge them via their SmartPhone as they go in and out of the store. 
There is no need for cash or any human interactions whilst in the store. 

The system is far more complex than the warehouse whereby in busy stores, cameras can have a blocked view or lighting can change and impact the 
algorithms. This is why there are only a few stores now whilst the experts attempt to build models that can account for these glitches. 

The number of sensors needed right now could also be costly which is another reason why they haven’t got them to a commercial release. 
Automated stores are most likely the way forward and Amazon will be the pioneers if so.

-	DeepLens

AWS DeepLens is a wireless-enabled video camera and development platform integrated with the AWS Cloud. 
It lets you use the latest AI tools and technology to develop computer vision applications based on a deep learning model.

Everything about the hardware is fully customisable for developers. For example, it is possible to create deep learning networks capable of 
recognising the objects in a room or even the faces of people in a room. 


###  <a id="Facebook"></a>Facebook

The bad press in the last 24 months with events like the Cambridge Analytica scandal has led to people questioning how Facebook use AI and data but 
the fact of the matter is they have been using it to great effect for a long time now.

With a dedicated research lab and billions of users, Facebook has an infrastructure designed for growth in AI.

-	Recommendations

A bit like Amazon, recommendations are high on the list of AI deployments at Facebook. As well as suggesting new friends, 
the items you see are your news feeds re all there because algorithms have told them to appear. 
Every time you do something on Facebook, it learns from your behaviour and can ensure it creates exactly the right intent next time around.

Businesses can use Facebook Ads to target the right users and you will see campaigns based on your activity and behaviours. 
One of the most impressive (some may say intrusive) aspects is re-targeting of products. 
If you’ve ever been shopping on a site and suddenly see the same item or similar items appear on Facebook, that is AI driven re-targeting. 
I promise Mark Zuckerberg isn’t listening to every conversation but deep learning models are getting very strong at tailoring these ads.

-	Content

With so much press around internet trolls and offensive posts, Facebook has algorithms that can alert them about content. 
A recent innovation was developing a script capable of spotting if teenagers were showing signs of depression and suicidal thoughts. 
This has been built for public good after some high-profile incidents that could have potentially been stopped. 

Other use cases are in terrorism and racial abuse.

-	Language

Facebook acquired Wit.ai, a natural language processing startup based in London. 
With the technology, they are able to decipher what their users are saying an analyse the context and meaning as well as the actual items themselves. 
The use case for this is tackling fake news and hate speech but it will also be used to better understand the behaviour and needs of their users.
 
Some analytics companies are using language to recognise personality traits. Many say these attributes are better factor of trustworthiness and 
integrity when it comes to things like credit. 
Trials are in place that use social data as opposed to traditional financial backgrounds for making loan or mortgage decisions.

The AI platform for understanding context is known s Deep Text. Facebook say it has “human like accuracy” of understanding the context of language 
through the deep analysis of intents and entities. 

-	Image Recognition

Millions of images are posted on Facebook. Identifying faces at automatically tagging them is one of the major advances machine learning algorithms 
have brought to the site over the years. This was trained using billions of photos from Instagram once they acquired it allowing them to build 
accurate models incredibly quickly. 

One of the most important use cases is that the algorithms can identify images and describe them to the visually impaired. 
This works by simply taking a photo and allowing the deep learning networks to quickly identify and explain them to users. 

Facebook even believe they have an algorithm which can work out the mood of a person based on their stature or pose in an image. 

-	Chatbots

Facebook Messenger is probably the most used conversational chatbot AI. 
Businesses are now able to let them customers service themselves or purchase products using the technology. 

Rich amounts of data feed the Messenger platform allowing it to conduct all kinds of activities. 
They have even given it the capability to negotiate with humans and it created its own way to bluff and lie in conversations.

 
###  <a id="Apple"></a>Apple

Many say that Apple is a bit of a follower when it comes to AI and machine learning with the other big companies doing much more in the way of 
leading innovation. In fact, even if we look at Siri which was the first voice assistant to enter the market, Amazon and Google quickly took over 
with their products when they had a lot more data available to make things happen faster.

The Apple model has typically relied on hardware rather than developing their own AI and machine learning models like the other companies in this 
article have done constantly. However, in April 2018, Apple hired John Giannandrea from Google. He was one of the core reasons why Google 
integrated AI into just about every product and a major part of their success. His appointment showed the direction that Apple desired to go in. 

With that, the latest iPhone models come with heaps of computational power and AI based algorithms. The objective of this was slicker camera 
effects and the ability to create amazing augmented reality (AR) experiences for the users. A little like Google and Amazon, customisation is 
key and non-developers are able to run their own algorithms using the Apple hardware. 

With non-developers having this sort of scope, the iTunes store is set to  livened up with new experiences for socialising and getting things done. 
The machine learning algorithms have image recognition to better understand photos as an example. The new computer chip in the iPhone is dedicated to 
running neural network software that starts to understand he concepts of speech as well as images, more so than ever before.

The new technology installed by Apple allows developers to run neural networks at a rate of 10 times faster than the iPhone X. 
To put this into some context, the basketball app HomeCourt has been able to improve drastically. The app analyses video and images to get on 
court analytics on shots, misses and dribbles. This used to take a couple of seconds to process but is now able to complete the analysis in 
real-time such is the computational power.

AI has become a key area of focus at Apple as they seek to keep up with Google, Amazon and Facebook. They have hired a number of staff to AI roles at 
the start of 2019 from developers to industry specific professionals. However, they don’t talk it up as loudly as the competitors.

In 2019, Apple announced that Siri is set to have a far more natural voice, personalised music will be available via their HomePod speaker system and 
Siri will be able to read incoming message to your AirPods. The Core ML platform is available to iOS developers for developing their own 
applications as we’ve already said and is continually improving. 

Apple have also hired former Google man Ian Goodfellow as the Director of Machine Learning to further exemplify their intent. 
However, it seems that although they have the resources and knowledge, the culture of the company remains in hardware and it will be hard to pull 
away from that completely. Some have suggested there are roadblocks in using machine learning given Apple’s commitment to privacy and that 
could be causing more problems with development than we realise. 

Although Apple are, and for the foreseeable future will be a huge power in the technology field, their AI strategy still seems to be somewhat 
lacking against their core competition. They need to think about releasing something big soon at the risk of falling short where 
Google, Amazon and Facebook are all powering ahead.

## The AI Startups Scene

###  <a id="Introduction"></a>Introduction

Artificial Intelligence (AI) investments and developments have grown in earnest since the start of the 21st century. 
With so many different applications across data, robotics, deep learning, IT and IoT, there is a huge opportunity for business offering innovative 
services. Hardly a day goes by where we don’t read about a new technology startup in the news or via social media. 
In fact, there is such a plethora of them in the market that it is becoming near on impossible to decipher which offer genuinely unique AI 
services and which are simply trying to cash in on the hype.  

###  <a id="TheStartupScene"></a>The startup scene

A survey by venture capitalist firm MMC at the start of 2019 found that of 2,830 startups in Europe that classified themselves as AI companies, 
only 1,580 accurately fit the description. The survey report goes on to say it is very important to research a company, their products, 
website a documentation before diving in at the deep end. 
It isn’t necessarily the fault of the company. They are not deliberately trying to deceive the public but third party analytics have 
classified them in that way. 

The issue here isn’t necessarily with the companies but the fact of the matter is those who are labelled as AI are attracting 15% to 50% more 
funding than other technology firms. Startups are aware of how they are classified but there is probably very limited incentive to correct listings 
when they are getting the investments they desperately need to survive. 

In 2019, one in twelve startups are now featuring AI as part of their products and services. Beyond that, more than 10% of large enterprises are 
using AI based applications as part of their business which is a growth of 4% in only 12 months. If that trend continues as more industries 
recognise the benefits of AI in promoting efficiency, reducing cost and increasing revenue, the startup market looks set to blossom.

###  <a id="WhatAreStartupsFocussingOn"></a>What are startups focussing on

Around one in four AI startups are primarily serving the company’s marketing department. This is where the sweet spot appears to sit with a wealth of 
data, variety of digital channels and the prospect of creating unique customer experiences. One of the most popular AI solutions is the conversational 
chatbot. Several startups are using this as their base as firms try to use natural language processing to understand their customers. 
Beyond this, in utilising a chatbot, businesses can reduce call centre costs and open a 24/7 customer service portal. 
There are a huge number of benefits which is why startups are swooping in. 

AI startups tend to cluster around where data resides. For example, we have seen a lot of traction in healthcare, finance, retail and entertainment 
because there is a vast volume of consumer information within those. Data is key to AI given that most solutions involve applications like machine 
learning and natural language processing. Some of the most significant areas are image recognition, tech to speech and making predictions or 
decisions akin to the recommendation engines you see Amazon or Netflix using. 

###  <a id="TheMostSuccessfulAIStartups"></a>The most successful AI startups

The list below are the top 10 AI startups in the US ranked by funding amount, published in February 2019.

10. Dataiku - $147 million

9. Landing AI - $175 million

8. Signifyd - $206 million

7. Pony.ai - $214 million

6. Data Robot - $225 million

5. C3 - $243 million

4. Butterfly Network - $350 million

3. UiPath - $448 million

2. Automation Anywhere - $550 million

1. Zymergen - $574 million

Worldwide, the best funded startups are both from China; SenseTime and Face++. 

SenseTime has had far and away more investment than any other and focusses on facial recognition technology. 
It has had significant government support. The solutions are heavily used in surveillance with support for police bureaus in identifying faces
 or looking for car number plates. 
 
Toutaio, another Chinese based company is worth circa $3.1B having been founded in 2012. The platform was the first of its kind to take content 
based on keywords and feed it to readers via an application. It gradually learns what readers like through analysing social media accounts.

At fourth in the US funding list, Butterfly Network also have the highest number of patents filed for their portable, hand-held ultrasound that has 
been built for under $2k. It uses computer vision AI to help hardware interpret the images. 

Some of the largest growth has been seen in the self-serving AI platforms like DataRobot and Dataiku. These companies provide platforms to 
business who don’t have expert resource but still want to develop AI and machine learning projects. Where resource is thin on the ground, 
businesses are starting to rely on such startups to get them going. 

The list could be endless and a quick Google search for Top AI companies gives you a different list in virtually every article. 
It is worth reviewing which ones can work best for you and ensuring they offer genuine AI potential and not simply hype.


###  <a id="Summary"></a>Summary

It is no secret that AI is big business. With so many startups in the space, it is important to be strategic and pick the right ones for you. 
Many will be industry specific and you must be sure they can adapt to ever changing business needs in a fast-paced digital economy. 
AI is where the future lies. 

 
## Autonomous Drive and AI

###  <a id="Introduction"></a>Introduction

When thinking of autonomous cars, many of us jump straight to David Hasselhoff talking to Kit in the 1980s 
(for the Millennials, just Google Knight Rider!).  This type of driverless car is the dream and, depending on which report you read, 
some think we are not too far away from commercial use of fully autonomous vehicles. 

However, whilst massive investment has helped progression no end, the arrival of a true driverless car that doesn’t need any type of 
human interaction doesn’t look to be around the corner just yet. Elon Musk (co-founder and CEO of Tesla) claims that Teslas will have 
full self-driving capability by the end of 2020. However, experts say that the technology is still too unpredictable and expensive with the 
chance of cars being able to navigate as a human would being near impossible. John Krafick, the CEO of Waymo (Google’s self-driving car project) 
echoes the fact that autonomy will have some restraints.  

Instead of thinking about fully automated vehicles, it is perhaps best to see the technology in stages of automation. 

•	Level 1 automation 

	some small steering or acceleration tasks are performed by the car without human intervention, but everything else is fully under human control

•	Level 2 automation 

	is like advance cruise control or original autopilot system on some Tesla vehicles, the car can automatically take safety actions but the 
	driver needs to stay alert at the wheel

•	Level 3 automation 

	still requires a human driver, but the human is able to put some “safety-critical functions” to the vehicle, under certain traffic or 
	environmental conditions. This poses some potential dangers as humans pass the major tasks of driving to or from the car itself, 
	which is why some car companies (Ford included) are interested in jumping directly to level 4

•	Level 4 automation 

	is a car that can drive itself almost all the time without any human input, but might be programmed not to drive in unmapped areas or during 
	severe weather. This is a car you could sleep in.

•	Level 5 automation 

	means full automation in all conditions

Whilst a level 5 develop is somewhere in the future, hitting a level 3 or 4 automation project could be well within the grasp of the leading companies. 
In fact, this is most likely what Elon Musk is referring to with his claims. We already have driver free shuttles operating in cities like 
Detroit, driverless university campus run-arounds and self-driving machines on farms to name a few successful pilots. 

Within these stages could be several other AI developments outside of just autonomy. For example, connected vehicles will rely on vast amounts of data. 
With hundreds of sensors applications of AI will be able to alert us of any problems before they happen, and we’re left stranded in the middle of a Highway. 

There are even potential applications in marketing. In the digital age, marketing is all about data. Just imagine if social media feeds 
flag that someone is going on holiday and whilst in their car, they are recommended the best travel money exchange stores near them. 
It could even flag restaurants as they drive past them. AI can know exactly what a driver needs and wants. 

Other possible uses for AI are in risk and manufacturing.

Waymo, Tesla and Baidu Apollo are three of the main players in the driverless vehicle industry. 
Below are the key developments and progress for each of them. 

###  <a id="Waymo"></a>Waymo

In May 2019, Waymo announced a partnership with Lyft whereby they would deploy 10 vehicles in the Phoenix area. 
Formerly known as the Google Self Driving Car Project set up in 2009, Waymo’s mission is to make it safe and easy for people and things to move around. 
They believe that fully self-driving technology can both improve mobility and give people the freedom to get around whilst saving thousands of 
lives by negating traffic crashes. “We’re not building a car, we’re building a driver.”

Waymo design all the core components of their technology in-house and see themselves as being able to advance vehicles much faster than flashier 
competitors like Tesla or Uber. Their sensors have a high amount of computational power, hence the rather large exterior but they value 
functionality more than they do looks. 

A foundation of machine learning from its association to Google Alphabet gives Waymo a very solid infrastructure.

###  <a id="Tesla"></a>Tesla

Probably the most well-known company in the world of autonomous and electric vehicles, Tesla have progressed the field significantly in the last decade.
 
Elon Musk stated at the start of 2019 that all vehicles now have the hardware installed to be fully self-driven. All that is required is that they 
improve the software but there is a long way to go with this.

Right now, Tesla vehicles are only considered to be at Level 2 automation. This is a more advanced assistance system than most other vehicles 
currently on the road. Whilst Musk has promised this will improve, it is unlikely this can get to Level 5 in the near future as he predicted. 
He says that by the middle of 2020, Tesla’s autonomous system will have improved to the point where drivers will not have to pay attention to the road.
 
We should note that Musk originally said he would have Level 5 vehicles on the road by the end of 2018. Tesla are definitely helping to drive the 
technology but common expectation is that it will be a good few years yet, but Musk’s claims are realised. 

###  <a id="BaiduApollo"></a>Baidu Apollo


The Baidu Apollo platform is an open source self-driving vehicle technology system. It provides a hardware and software solution including cloud 
data services as well as a vehicle hardware platform. Baidu offers the source code and capabilities in obstacle perception, trajectory planning, 
vehicle control and operating systems. 

In 2019 Baidu announced that Apollo Enterprise for vehicles will be put into mass production. It is already being used by 130 partners around the 
world and one of its Chinese users plan to deploy 3 autonomous vehicles by 2021. 

The Apollo 3.5 release now supports “complex urban and suburban driving environments” pushing it closer to Level 5 automation capability. 
It is already being utilised by companies and piloted by Walmart for grocery deliveries. 

Baidu has grand plans such as 100 robo-taxis covering 130 miles of city in China which are all able to communicate with the road infrastructure like 
traffic lights. 

###  <a id="WhereNextWithAutonomousVehicles"></a>Where next with autonomous vehicles?

When companies talk about autonomous vehicles they are probably being quite optimistic to ensure they get the required investment and 
interest in the future of the industry. We also need to be aware of potential regulatory developments for autonomous vehicle deployment such as 
who is liable if something does go wrong e.g. an accident on a busy road. 

It is very likely that within the next decade we will have some sort of truly autonomous vehicle in a city somewhere if companies can handle 
the scepticism. 
The level 3 and 4 automation will definitely continue to prosper as AI becomes more a part of everyday life and it is expected 
this will become available in the majority of city environments. 


