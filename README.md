
Artificial Intelligence for Business Leaders

[![GitPitch](https://gitpitch.com/assets/badge.svg)](https://gitpitch.com/fractus-io/ai4bl/master?grs=github&t=white)


Table of Contents

#### Introduction

* [What is AI]
  * [What is Artificial Intelligence](#WhatIsArtificialIntelligence)
  * [Applications of AI](#ApplicationsOfAI)
  * [Companies that use AI](#CompaniesThatUseAI)
  * [The future prospect of AI](#TheFutureProspectOfAI)
* [History of AI]
  * [Introduction](#hoai-Introduction)
  * [The rise of robotics and AI](#TheRiseOfRoboticsAndAI)
  * [The AI Winters](#TheAIWinters)
  * [New Opportunities for AI](#NewOpportunitiesForAI)
* [Future of the AI]
  * [Introduction](#foai-Introduction)
  * [The future of AI – beyond expectation](#TheFutureOfAIBeyondExpectation)
  * [Computing Power](#ComputingPower)
  * [Emerging Technology](#EmergingTechnology)
  * [Assistants will become predictive](#AssistantsWillBecomePredictive)
  * [Affective Computing](#AffectiveComputing) 
  * [Changing Professions](#ChangingProfessions) 
  * [Let’s get digital](#LetsGetDigital)

#### Understand AI

* [AI, Machine Learning and Deep Learning](/tutorials/ai4bl/ai-machine-learning-and-deep-learning)
* [Machine Learning Algorithms](/tutorials/ai4bl/machine-learning-algorithms)
* [Neural Networks](/tutorials/ai4bl/neural-networks)
* [Why AI is taking off](/tutorials/ai4bl/why-ai-is-taking-off)
* [How AI can benefit your organization](/tutorials/ai4bl/how-ai-can-benefit-your-organization)

#### Who is who in AI

* [The Godfathers of AI](/tutorials/ai4bl/the-godfathers-of-ai)
* [Big Companies and AI](/tutorials/ai4bl/big-companies-and-ai)
* [The AI Startups Scene](/tutorials/ai4bl/the-ai-startups-scene)
* [Autonomous Drive and AI](/tutorials/ai4bl/autonomous-drive-and-ai)

#### AI Use Cases

* [Computer Vision](/tutorials/ai4bl/computer-vision)

#### How AI is changing industry

* [Using Data and AI in Telecoms](/tutorials/ai4bl/ai-telecoms)
* [Using AI in Media](/tutorials/ai4bl/ai-media)
* [Using AI in Healthcare](/tutorials/ai4bl/ai-healthcare)
* [AI in Agriculture](/tutorials/ai4bl/ai-agriculture)
* [AI in Manufacturing](/tutorials/ai4bl/ai-manufacturing)
* [AI in Retail](/tutorials/ai4bl/ai-retail)
* [AI in Transportation](/tutorials/ai4bl/ai-Transportation)
* [AI in Finance](/tutorials/ai4bl/ai-finance)
* [How AI is changing the travel industry](/tutorials/ai4bl/ai-travel)
* [How can IoT AI assist in improving life quality for people with special needs](/tutorials/ai4bl/ai-iot) 


## What is AI

###  <a id="WhatIsArtificialIntelligence"></a>What is Artificial Intelligence 

When people talk about artificial intelligence (AI), their mind often goes straight to the movies. 
They seem to think that some company is going to create Skynet and manufacture Terminators to initiate the extinction of humanity. 
This is a common misperception of AI whereby movies have associated it with sci-fi and fantasy but in reality, it is quite different. 


AI is the use of computer science and programming that trains machine to imitate human tasks and thought processes. 
It works by analysing data and surroundings to solve problems, incrementally learning for itself to continually improve. 
AI functions are initially based or trained using the instructions given to it by humans and when these are a bit vague or incomplete, 
in theory we could get Skynet type consequences (albeit not quite so extreme).


Right now, all forms of AI use some sort of human intervention. That could be loading the training data or analysing the results and perfecting them. 
AI is not at a point yet where it has its own conscious decision-making process and can see the world as or better than a human would. 
This is likely to still be quite a long way in the future and it is important not to exaggerate the capabilities. 


Instead of talking about AI, discussing its applications helps to make better sense of the term and show how it is impacted many parts of everyday life.
 

###  <a id="ApplicationsOfAI"></a>Applications of AI 

You are probably exposed to AI every day. Whether it be using Facebook Messenger, talking to Alexa, watching Netflix, listening to Spotify or 
searching on Google, you are using a form of AI. Most of these examples are powered by an AI application known as machine learning. 


Machine learning is the use of existing data to make future decisions. Algorithms built without programming platforms are designed to enable 
machines to make unsupervised choices based on the data they have been supplied with. 
One of the best ways to explain machine learning is when comparing to how a baby learns to walk. 


A baby would start by taking in the surroundings and watching other children or adults walking. Nobody explicitly tells a baby to move their left foot 
forward, then the right, then the left again, then the right and so on. 
In gathering data from the environment, a baby will learn for themselves and attempt their first steps. 
Initially, they might fail so next time they use a table to help them up. Over time, the baby connects all the dots provided by data and begins to walk.

 
A machine will learn in the same way. Let’s say you want the machine to separate pictures of cats from pictures of dogs. 
To start, you give it a large collection of cat photos and it looks through to find the patterns. 
When it is presented with a new photo, it tries to work out whether it is a cat or dog. Every time the machine fails, it learns from the mistake and 
becomes more accurate. It can do this against vast volumes of data. In theory, the machine should become 100% accurate with the task.

 
Machine learning works using data so a human never has to program it. It might even find patterns that a human never would have done. 
A real-world example in Hong Kong has shown that machine learning has become more accurate than doctors in cancer diagnosis through analysing i
mages of patients who have symptoms. 


The digital world is full of data meaning machine learning has a major part to play. One of the key applications today is in conversational
chatbots which use a form of machine learning known as natural language processing. 
Amazon Alexa can take voice commands and analyse them against a huge knowledge base to return the most relevant response to the user. 
Sensors on factory machines are being used to constantly record data and predict when maintenance could be needed before any problems arise. 
In contract law, algorithms can review thousands of articles simultaneously and potentially solve cases in a split second that would normally 
take a human weeks or months. 
 
 
Those are just some applications, but it exemplifies the value of treating data as a business asset. 
AI is more about data than it is about the fantasy we see in the movies.



###  <a id="CompaniesThatUseAI"></a>Companies that use AI 

AI applications are used in almost every company that we interact with. Here are a few popular examples.

-	Google – every Google search uses machine learning. 
			 It takes what the user writes or says and applies that to algorithms, returning the most relevant results. 
			 Google can even do this with video now as the AI has advanced over several iterations since 2010. 

-	Netflix – the streaming service users what we would call a recommender system. Instead of users choosing a show to watch, 
			  Netflix uses data to predict what their subscribers will want to watch next. 
			  Recent statistics have suggested as much as 80% of user choices have come via the recommendations. 
			  Subscribers are even presented with different show thumbnails based upon their likely preferences. 
			  Spotify and Amazon use similar models

-	Facebook – the social network is massively based on data. 
			   Users get ads based on their preferences and it is never just a coincidence when they are relevant. 
			   Messenger is now a conversational chatbot used by major companies to complete retail actions without any human involvement. 
			   Facebook has utilised the purchase of WhatsApp to get a great understanding of how people converse. 

###  <a id="TheFutureProspectOfAI"></a>The future prospect of AI 

At the moment, we are only really at the start of an AI revolution. 
Applications like machine learning are still relatively new outside the big enterprises and have only been deployed as a very light touch. 
However, that said, many of the applications seem so normal that we don’t even remember they exist. 
Talking to Alexa to turn on your lights has become standard in some households. 
In the future, the same will most likely happen with driverless cars and robotics but we are a little way off that yet.   

What is becoming quite scary is that humans are beginning to trust AI applications more than another human. 
In fact, even when in a retail store, over 60% of people surveyed said they would rather use their SmartPhone to answer questions than ask a human a
ssistant. As new technology like driverless cars come into play, this lack of trust could start a societal breakdown of sorts which is perhaps 
why we are holding back just a bit with such game changers. 

## History of AI

###  <a id="hoai-Introduction"></a>Introduction

We tend to see artificial intelligence (AI) as a brand new development but the history books would tell us otherwise. 
No longer the domain of science fiction, robotics and artificial intelligence and becoming important business drivers. 
This article looks at how we got to where we are today. 


The timeline below from the University of Queensland gives a brief overview of how AI has progressed over the years into becoming a standard 
part of university offerings. 

Whilst this timeline provides us wit a great overview, we are going to start back at 1921 at a time when the term “robot” was first used.


###  <a id="TheRiseOfRoboticsAndAI"></a>The rise of robotics and AI

The term robot was first used by Czech writer Karel Capek almost 100 years ago in 1921, although he credited his brother Josef Capek as being the 
inventor of the word. It comes from the word robota which is associated with labor or work in Slovak and gives us an insight into its intention. 


In 1939, a humanoid robot named Elektro was presented at the World Fair, smoking cigarettes and blowing up balloons for the audience. 
This was a couple of years before Isaac Asimov formulated his “three laws of robotics” that most of us will be familiar with from movies 
like “I, Robot.” The three laws of robotics state:

1.	A robot may not injure a human being or through inaction allow a human being to be harmed

2.	A robot must obey orders given to it by human beings except where such orders would conflict with the first law

3.	A robot must protect its own existence as long as such protection does not conflict with the first or second law

These laws still stand firm today and in their own way are built into artificially intelligent devices. 


Moving into the 1940s and 1950s and the foundations of neural networks in machines start to be developed.  
In many papers, this period is considered to be the true start of AI as computer science started being used to solve real world problems, 
moving away from just theory and fantasy. 

During the Second World War, the British computer scientist worked to crack the “Enigma” code which was used by German forces to send secure messages. 
This was done by Turing and his team using the Bombe machine and laid down the foundations for the application of machine learning; 
using data to imitate human tasks. 

Turing was amongst the first to consider that a machine could converse with a human, without the human knowing it was a machine. 
Many know this as the “imitation game”, another that has since been made into a popular movie. 

The standard was set for AI and in the 50s and 60s, research into the domain began to boom. 
In 1951, Marvin Minsky built the first neurocomputer and a machine known as Ferranti Mark 1 successfully used an algorithm to master checkers. 
At a similar time, John McCarthy who is often penned at the father of AI, developed the LISP programming language which has become very important 
in machine learning.


In the 1960s, there was more exploration around robotics as GM installed the Unimate robot to lift and stack hot pieces of metal. 
Frank Rosenblatt also constructed the Mark I Perception computer which was able to learn skills by trial and error. 
By 1968, a mobile robot known as “Shakey” is introduced and controlled by a computer the size of a room. 


###  <a id="TheAIWinters"></a>The AI Winters

Despite all this progress, during the 1970s, AI hit a period known as the AI Winters, coined as an analogy of the nuclear winter. 
Scientists were finding it very difficult to create truly intelligent machines as there simply wasn’t enough data to do so. 
This led to a slide in government funding as confidence began to dwindle. 


Research slowed until the 1990s apart from a few notable projects such a SCARA, a robotic arm invented for assembly lines in 1979 and 
research by Doug Lanat and his team in the 80s which looked to codify what we call human common sense. 
Also, in 1988, the first version of a conversational chatbot was launched and we saw a service bot in hospitals for the first time. 
Some of these developments created a bit of a spark and a renewed interest in the potential of AI going into the 1990s. 



###  <a id="NewOpportunitiesForAI"></a>New Opportunities for AI

During the 90s and going into the new Millennium, companies started showing a new interest in AI and it had a second coming of sorts. 
The Japanese government announced plans to develop a new generation computer to advance machine learning. 
In 1997, IBM’s Deep Blue computer famously defeated world chess champion, Gary Kasparov and really propelled AI into the limelight. 


Improvements in computer hardware meant companies had more data and therefore, greater opportunity to develop machine learning propositions. 
In 1999, although we were a way off seeing the likes of Pokemon Go, augmented reality was first coined as a framework and term. 
It seems as if these theories started to drive a wave of developments in the early 2000s as the Big 4 (Google, Amazon, Facebook and Apple) 
gained a major share of the AI market. 


In 2005, autonomous cars took a huge leave, driving 183 miles without intervention. 
IBM introduces their Watson AI assistant in 2006 which later defeated a Jeopardy champion in the US. 
Google launched street view in 2010 and in 2011 we met Siri for the first time. 
It wasn’t until 2015 that Alexa finally hit the market place and then drone deliveries started, Google lens became a reality, 
smart homes were the in thing and people started having their own Virtual Reality platforms at home.


In fact, for all the history of AI, the last ten years have been huge, creating so much data and allowing us to change life as we know. 
It is difficult to imagine life without AI in the modern digital world. This is quite an amazing feat when we considered the tumultuous times of the 
last century.

## Future of the AI

###  <a id="foai-Introduction"></a>Introduction 

Artificial Intelligence (AI) is not a new innovation, although it certainly feels like it. 
Its origins are usually dated back to the 1920s when the term “robot” was first coined but it came into prominence during the Second World War 
thanks to Turing and the cracking of the Enigma code.  
Between the 1970s and the new Millennium, it struggled to gain traction as there simply wasn’t enough data for it to develop meaning nobody was 
willing to invest. However, the 21st century has seen a huge resurgence and since 2010, AI has dramatically changed everyday life. 

If we think about some of the things we do today, many didn’t even exist 10 or 15 years ago. 
We use Smartphones, check our social media, search using Google, talk to Alexa, stream on Netflix, listen to music on Spotify, 
buy products from Amazon, turn our lights from an App, catch Pokemon on the streets, play virtual reality games from our home, 
get an Uber or order food from Deliveroo. All these things are standard and show how far we have come in a short space of time. 
Every single one of them has a foundation of data and artificial intelligence. 

Whilst AI has brought us these amazing innovations, they are not that exciting anymore. If we expect Alexa to respond to voice commands, 
the fact that the technology is starting to do that better is great but not really a new development. 
A lot of the technology we have talked about falls into a machine learning application, ones that use data to make decisions. 
Google returns webpages that it predicts are most likely to be relevant and Netflix uses data to give us the shows it predicts we will like. 
This idea has become an expectation rather than an innovation. So, what’s next?


###  <a id="TheFutureOfAIBeyondExpectation"></a>The future of AI – beyond expectation

Experts in the field are primarily focusing on artificial general intelligence (AGI). 
Am AGI machine is one which can perform any cognitive task that a human can. Whilst the technology we have is amazing and useful, it is not cognitive, 
it does not have a concept of the world or a conscious mind if you will. 
Existing AI would not be able to pass the Turing test in which a computer must prove its intelligence as indistinguishable from that of a human. 
That is the goal for AGI and the future aims to move us closer to that point. 
Many believe that could be in the next 5 to 10 years whereas other would suggest 20 to 30 years is more accurate. 

A machine with AGI would be able to perform any task that a human being can do without being programmed to do so. 
For example, if asks to hammer a nail it would simply start guessing at a few things and continue to fail until it got it right. 
The basis of AGI would be trial and error, the same as when a human learns to walk for example. 
We should probably remember that scientists don’t even fully understand the human brain yet, let alone training a machine to do the same.


###  <a id="ComputingPower"></a>Computing Power 

One of the limitations to further development has been the lack of computer power available. 
Some of the aforementioned slow periods have also been caused by a lack of data to work with. 
For AI to get close to the intelligence of a human brain, we need what is known as quantum computing. 
Whilst the technology has been released by the likes of IBM, it is still very much in its infancy and we don’t know if they will become 
mainstream quickly. 

What this means is that whilst AI might be able to look at images of cars and build the models, it is some way off being creative enough to 
come up with its own ideas for building a car like a human would. 

###  <a id="EmergingTechnology"></a>Emerging Technology

Whilst AGI might not be imminent, advances in cloud technology, edge computing and Big Data platforms should bridge the gap between AI and robotics. 
These technologies are helping to process data faster and more effectively meaning robots can make better decisions and become more useful. 
For example, AI powered robots can carry out dangerous tasks safe in the knowledge they know what they are doing through existing datasets. 
Machine learning won’t just power Google, Facebook and Netflix but also more practical events that make our lives safer. 

###  <a id="AssistantsWillBecomePredictive"></a>Assistants will become predictive


As it stands, Alexa and Google Home respond to the commands we give them. What if they were able to start predicting what we needed? 
The same applies to other smart home devices. For example, a smart refrigerator can work out when you run out of milk and order some to be 
delivered to your day without any intervention. 
Instead of asking Alexa to turn off the lights, smarter assistants will know when you need events to happen. 
As assistants gather data, they are becoming intelligent enough to be predictive.

###  <a id="AffectiveComputing"></a>Affective Computing 


Emotional intelligence is something that AI traditionally lacks. If you say the same thing to Alexa in a happy tone or a sad tone, 
you receive the same response. Affective computing is a field that studies speech or body language and images to recognize our needs. 
For example, conversational chatbots can respond differently based on how we speak or type. 
This is done by monitoring pauses on speech or pitch of voice as a measure of emotion. The aim is to start making devices appear more human. 

###  <a id="ChangingProfessions"></a>Changing Professions 

Given the fact that AI can diagnose medical conditions through image scanning and data we will start to see changes in healthcare. 
Doctors will be able to care for patients rather than spending time on diagnosing them because AI can carry out the task in a split second. 
AI can also review contracts, provide measurements for construction or pay out insurance claims. 
Professional interactions will slowly evolve over the next few years. 

###  <a id="LetsGetDigital"></a>Let’s get digital 

Let’s get digital
Everything will be digital. Technology such as Blockchain can be used to verify our identity so we don’t need to carry ID cards. 
The movement towards cryptocurrency like Bitcoin will continue. 
Ultimately, we will interact digitally making everyday life far more efficient than it is today. 
